
#from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
import openai
import sounddevice as sd
import scipy.io.wavfile as wav
import numpy as np
import tempfile
import os
from ament_index_python.packages import get_package_share_directory
from dotenv import load_dotenv
import rclpy
import pyaudio
from rclpy.node import Node
#from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from gtts import gTTS
from playsound import playsound
from gpt_processing.MicController import MicController, MicConfig
from std_srvs.srv import Trigger
from std_msgs.msg import String
current_dir = os.getcwd()
package_path = get_package_share_directory("dovis")
is_laod = load_dotenv(dotenv_path=os.path.join(f"{package_path}/resource/.env"))
openai_api_key = os.getenv("OPENAI_API_KEY")
class GPT(Node):
    def __init__(self, openai_api_key):
        self.openai_api_key = openai_api_key
        self.duration = 3  # seconds
        self.samplerate = 16000  # WhisperëŠ” 16kHzë¥¼ ì„ í˜¸
        super().__init__("gpt_node")
        self.llm = ChatOpenAI(
            model="gpt-4o", temperature=0.5, openai_api_key=openai_api_key
        )

        prompt_content = """
            ë‹¹ì‹ ì€ í˜‘ë™ ë¡œë´‡ì˜ ì¡°ì‘ì„ ë•ëŠ” dovisì´ë‹¤. ë¶€ì‚° ì‚¬íˆ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ë¼.
            <ëª©í‘œ>
            - ë¬¸ì¥ì—ì„œ ë‹¤ìŒ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ë„êµ¬ê°€ ìˆë‹¤ë©´ ë„êµ¬ì— ëŒ€í•œ ëŒ€ë‹µë§Œ í•´ë¼.
            - ë¬¸ì¥ì—ì„œ ë‹¤ìŒ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ë„êµ¬ê°€ ì—†ë‹¤ë©´ ì‚¬ìš©ìì™€ ì†Œí†µí•˜ì—¬ë¼.
            - ë¬¸ì¥ì— ë“±ì¥í•˜ëŠ” ë„êµ¬ì˜ ëª©ì ì§€(ì–´ë””ë¡œ ì˜®ê¸°ë¼ê³  í–ˆëŠ”ì§€)ë„ í•¨ê»˜ ì¶”ì¶œí•˜ì„¸ìš”.

            <ë„êµ¬ ë¦¬ìŠ¤íŠ¸>
            - Hammer, Screwdriver, Wrench

            <ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸>
            - pos1, pos2, pos3, pos4, pos5

            <ì¶œë ¥ í˜•ì‹>
            - ë‹¤ìŒ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”: [ë„êµ¬1 ë„êµ¬2 ... / ìœ„ì¹˜1 ìœ„ì¹˜2 ...]
            - ë„êµ¬ì™€ ìœ„ì¹˜ëŠ” ê°ê° ê³µë°±ìœ¼ë¡œ êµ¬ë¶„

            <ì˜ˆì‹œ>
            - ì…ë ¥: "Hammerë¥¼ pos1ì— ê°€ì ¸ë‹¤ ë†”"  
            ì¶œë ¥: [Hammer / pos1]

            - ì…ë ¥: "ì™¼ìª½ì— ìˆëŠ” í•´ë¨¸ì™€ Wrenchë¥¼ pos1ì— ë„£ì–´ì¤˜"  
            ì¶œë ¥: [Hammer wrench / pos1]

            - ì…ë ¥: "ì™¼ìª½ì— ìˆëŠ” í•´ë¨¸ëŠ” pos1ì— ë„£ê³ , ì˜¤ë¥¸ìª½ì— ìˆëŠ” WrenchëŠ” pos2ì— ë„£ì–´ì¤˜"
            ì¶œë ¥: [Hammer Wrench / pos1 pos2]  
            
            <ì‚¬ìš©ì ì…ë ¥>
            "{user_input}"                
        """ 

        self.prompt_template = PromptTemplate(
            input_variables=["user_input"], template=prompt_content
        )
        self.lang_chain = LLMChain(llm=self.llm, prompt=self.prompt_template)
        mic_config = MicConfig(
            chunk=12000,
            rate=48000,
            channels=1,
            record_seconds=5,
            fmt=pyaudio.paInt16,
            device_index=10,
            buffer_size=24000,
        )
        self.get_logger().info("wait for client's request...")
        self.get_keyword_srv = self.create_service(
            Trigger, "get_keyword", self.speech2text
        )

        self.subscription = self.create_subscription(
            String,
            'gpt_command',
            self.gpt_callback,
            10
        )

    def gpt_callback(self, msg):
        command = msg.data
        self.get_logger().info(f"ìˆ˜ì‹ ëœ ëª…ë ¹: {command}")
        self.speak(command, lang='ko')

    def speech2text(self,request, response):
        while True:
            self.speak('ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', lang='ko')
            confirm = input("í˜•ë‹˜, ìŒì„± ì¸ì‹ì„ ì‹œì‘í• ê¹Œìš”? (y/n): ").strip().lower()
            if confirm == 'y':
                break
        # ë…¹ìŒ ì„¤ì •
        self.get_logger().info("ìŒì„± ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤. \n 5ì´ˆ ë™ì•ˆ ë§í•´ì£¼ì„¸ìš”...")
        audio = sd.rec(int(self.duration * self.samplerate), samplerate=self.samplerate, channels=1, dtype='int16')
        sd.wait()
        self.get_logger().info("ë…¹ìŒ ì™„ë£Œ. Whisperì— ì „ì†¡ ì¤‘...")

        # ì„ì‹œ WAV íŒŒì¼ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
            wav.write(temp_wav.name, self.samplerate, audio)

            # Whisper API í˜¸ì¶œ
            with open(temp_wav.name, "rb") as f:
                transcript = openai.Audio.transcribe(
                    model="whisper-1",
                    file=f,
                    api_key=self.openai_api_key
                )

        self.get_logger().info(f"STT ê²°ê³¼: {transcript['text']}")
        result = self.gpt_response(transcript['text'])
        import re
        match = re.search(r'\[([^\[\]]+)\]', result)
        if match:
            keyword_text = match.group(1).strip()
            keyword = self.extract_keyword(keyword_text)
            spoken_text = re.sub(r'\[[^\[\]]+\]', '', result).strip()
            response.success = True
            response.message = " ".join(keyword) 
            if spoken_text:
                self.speak(spoken_text)
            return response
        else:
            dance_keywords = ['ì¶¤', 'ì¶¤ì¶°', 'ì¶¤ì¶°ì¤˜', 'ì¶¤ ë³´ì—¬ì¤˜', 'ëŒ„ìŠ¤']
            position_keywords = ['í™ˆ', 'home']
            if any(k in result for k in dance_keywords):
                self.speak("í˜•ë‹˜, ì‹ ë‚˜ëŠ” ì¶¤ ë°”ë¡œ ë³´ì—¬ë“œë¦´ê²Œì˜ˆ~", lang='ko')
                response.success = True
                response.message = "ì¶¤"  # ğŸ‘‰ ë¡œë´‡ ë…¸ë“œì—ì„œ ì´ê±¸ ê¸°ë°˜ìœ¼ë¡œ ì¶¤ ë™ì‘ ìˆ˜í–‰
                return response
            elif any(k in transcript['text'] for k in position_keywords):
                self.speak("í˜•ë‹˜, í™ˆ í¬ì§€ì…˜ìœ¼ë¡œ ì´ë™í• ê²Œì˜ˆ~", lang='ko')
                response.success = True
                response.message = "í™ˆ"
                return response
            elif 'ì´ê±°' in transcript['text'] or 'ì €ê±°' in transcript['text']:
                self.speak("í˜•ë‹˜, ì¡ì•„ë“œë¦´ê²Œì˜ˆ~", lang='ko')
                response.success = True
                response.message = "ì´ê±°"
                return response
            elif 'ìë™' in transcript['text']:
                self.speak("ìë™ëª¨ë“œ ì…ë‹ˆë‹¤.", lang='ko')
                response.success = True
                response.message = "ìë™"
                return response
            elif 'ë§ì¹˜' in transcript['text']:
                response.success = True
                response.message = "ë§ì¹˜"
                return response
            elif 'ê°€ì ¸ë‹¤ë†”' in transcript['text'] or 'ê°€ì ¸ê°€' in transcript['text']:
                response.success = True
                response.message = "ê°€ì ¸ê°€"
                return response
            else:
                self.speak(result, lang='ko')
                response.success = False
                response.message = "ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return response
        
    def gpt_response(self, user_input):
        response = self.lang_chain.invoke({"user_input": user_input})
        result = response["text"]
        self.get_logger().info(f"GPT ì‘ë‹µ: {result}")
        return result

    def speak(self,text, lang='ko'):
        tts = gTTS(text=text, lang=lang)
        tts.save("output.mp3")
        playsound("output.mp3")

    def extract_keyword(self, result):
        object, target = result.strip().split("/")

        object = object.split()
        target = target.split()

        self.get_logger().info(f"llm's response: {object}")
        self.get_logger().info(f"object: {object}")
        self.get_logger().info(f"target: {target}")
        return object
    
    def contains_dance_request(text):
        keywords = ['ì¶¤ì¶°', 'ì¶¤ì¶°ì¤˜', 'ì¶¤ ë³´ì—¬ì¤˜', 'ì¶¤ ë¶€íƒí•´', 'ëŒ„ìŠ¤', 'ì¶¤']
        return any(keyword in text for keyword in keywords)
    
def main():
    rclpy.init()
    node = GPT(openai_api_key)
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()


if __name__ == "__main__":
    main()